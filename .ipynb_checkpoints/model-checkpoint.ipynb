{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f36aa24",
   "metadata": {},
   "source": [
    "# Изтегляме някой необходими библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5e75344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\simona\\anaconda3\\lib\\site-packages (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\simona\\anaconda3\\lib\\site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\simona\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\simona\\anaconda3\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\simona\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: torch in c:\\users\\simona\\anaconda3\\lib\\site-packages (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\simona\\anaconda3\\lib\\site-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\simona\\anaconda3\\lib\\site-packages (4.26.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\simona\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\simona\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\simona\\anaconda3\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: requests in c:\\users\\simona\\anaconda3\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\simona\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\simona\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\simona\\anaconda3\\lib\\site-packages (from transformers) (0.12.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\simona\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\simona\\anaconda3\\lib\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\simona\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\simona\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\simona\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\simona\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\simona\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\simona\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\simona\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n"
     ]
    }
   ],
   "source": [
    "! pip install pandas\n",
    "! pip install torch\n",
    "! pip install transformers\n",
    "! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ffbfa6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2602793059.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Simona\\AppData\\Local\\Temp\\ipykernel_10312\\2602793059.py\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    import torch from torch\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from transformers import  BertModel, BertTokenizer, BertConfig, TFBertModel\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "import tensorflow.keras.backend as K \n",
    "from tensorflow.keras.layers import Dense, Input, Dropout,Embedding, GlobalAvgPool1D\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import  Model\n",
    "import tensorflow.keras.backend as K\n",
    "from keras.layers import Embedding\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "844ea313",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10312\\980380746.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cuda\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"cpu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a933c78a",
   "metadata": {},
   "source": [
    "# NAh loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de5fe575",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv('train.csv')\n",
    "test_dataset = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0257209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction Label</th>\n",
       "      <th>Title</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>One of the best game music soundtracks - for a...</td>\n",
       "      <td>Despite the fact that I have only played a sma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Batteries died within a year ...</td>\n",
       "      <td>I bought this charger in Jul 2003 and it worke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>works fine, but Maha Energy is better</td>\n",
       "      <td>Check out Maha Energy's website. Their Powerex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Great for the non-audiophile</td>\n",
       "      <td>Reviewed quite a bit of the combo players and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>DVD Player crapped out after one year</td>\n",
       "      <td>I also began having the incorrect disc problem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>Incorrect Disc</td>\n",
       "      <td>I love the style of this, but after a couple y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>DVD menu select problems</td>\n",
       "      <td>I cannot scroll through a DVD menu that is set...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>Unique Weird Orientalia from the 1930's</td>\n",
       "      <td>Exotic tales of the Orient from the 1930's. \"D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>Not an \"ultimate guide\"</td>\n",
       "      <td>Firstly,I enjoyed the format and tone of the b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>Great book for travelling Europe</td>\n",
       "      <td>I currently live in Europe, and this is the bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>Not!</td>\n",
       "      <td>If you want to listen to El Duke , then it is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>A complete Bust</td>\n",
       "      <td>This game requires quicktime 5.0 to work...if ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>TRULY MADE A DIFFERENCE!</td>\n",
       "      <td>I have been using this product for a couple ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>didn't run off of USB bus power</td>\n",
       "      <td>Was hoping that this drive would run off of bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>Don't buy!</td>\n",
       "      <td>First of all, the company took my money and se...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Prediction Label                                              Title  \\\n",
       "0                  2  One of the best game music soundtracks - for a...   \n",
       "1                  1                   Batteries died within a year ...   \n",
       "2                  2              works fine, but Maha Energy is better   \n",
       "3                  2                       Great for the non-audiophile   \n",
       "4                  1              DVD Player crapped out after one year   \n",
       "5                  1                                     Incorrect Disc   \n",
       "6                  1                           DVD menu select problems   \n",
       "7                  2            Unique Weird Orientalia from the 1930's   \n",
       "8                  1                            Not an \"ultimate guide\"   \n",
       "9                  2                   Great book for travelling Europe   \n",
       "10                 1                                               Not!   \n",
       "11                 1                                    A complete Bust   \n",
       "12                 2                           TRULY MADE A DIFFERENCE!   \n",
       "13                 1                    didn't run off of USB bus power   \n",
       "14                 1                                         Don't buy!   \n",
       "\n",
       "                                              Content  \n",
       "0   Despite the fact that I have only played a sma...  \n",
       "1   I bought this charger in Jul 2003 and it worke...  \n",
       "2   Check out Maha Energy's website. Their Powerex...  \n",
       "3   Reviewed quite a bit of the combo players and ...  \n",
       "4   I also began having the incorrect disc problem...  \n",
       "5   I love the style of this, but after a couple y...  \n",
       "6   I cannot scroll through a DVD menu that is set...  \n",
       "7   Exotic tales of the Orient from the 1930's. \"D...  \n",
       "8   Firstly,I enjoyed the format and tone of the b...  \n",
       "9   I currently live in Europe, and this is the bo...  \n",
       "10  If you want to listen to El Duke , then it is ...  \n",
       "11  This game requires quicktime 5.0 to work...if ...  \n",
       "12  I have been using this product for a couple ye...  \n",
       "13  Was hoping that this drive would run off of bu...  \n",
       "14  First of all, the company took my money and se...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.columns = ['Prediction Label','Title','Content']\n",
    "test_dataset.columns = ['Prediction Label','Title','Content']\n",
    "\n",
    "\n",
    "train_dataset.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d52bcd09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction Label</th>\n",
       "      <th>Title</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>The best soundtrack ever to anything.</td>\n",
       "      <td>I'm reading a lot of reviews saying that this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Amazing!</td>\n",
       "      <td>This soundtrack is my favorite music of all ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Excellent Soundtrack</td>\n",
       "      <td>I truly like this soundtrack and I enjoy video...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "      <td>If you've played the game, you know how divine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>an absolute masterpiece</td>\n",
       "      <td>I am quite sure any of you actually taking the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>Buyer beware</td>\n",
       "      <td>This is a self-published book, and if you want...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>Glorious story</td>\n",
       "      <td>I loved Whisper of the wicked saints. The stor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>A FIVE STAR BOOK</td>\n",
       "      <td>I just finished reading Whisper of the Wicked ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>Whispers of the Wicked Saints</td>\n",
       "      <td>This was a easy to read book that made me want...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>The Worst!</td>\n",
       "      <td>A complete waste of time. Typographical errors...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>Great book</td>\n",
       "      <td>This was a great book,I just could not put it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>Great Read</td>\n",
       "      <td>I thought this book was brilliant, but yet rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>Oh please</td>\n",
       "      <td>I guess you have to be a romance novel lover f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>Awful beyond belief!</td>\n",
       "      <td>I feel I have to write to keep others from was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>Don't try to fool us with fake reviews.</td>\n",
       "      <td>It's glaringly obvious that all of the glowing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Prediction Label                                              Title  \\\n",
       "0                  2              The best soundtrack ever to anything.   \n",
       "1                  2                                           Amazing!   \n",
       "2                  2                               Excellent Soundtrack   \n",
       "3                  2  Remember, Pull Your Jaw Off The Floor After He...   \n",
       "4                  2                            an absolute masterpiece   \n",
       "5                  1                                       Buyer beware   \n",
       "6                  2                                     Glorious story   \n",
       "7                  2                                   A FIVE STAR BOOK   \n",
       "8                  2                      Whispers of the Wicked Saints   \n",
       "9                  1                                         The Worst!   \n",
       "10                 2                                         Great book   \n",
       "11                 2                                         Great Read   \n",
       "12                 1                                          Oh please   \n",
       "13                 1                               Awful beyond belief!   \n",
       "14                 1            Don't try to fool us with fake reviews.   \n",
       "\n",
       "                                              Content  \n",
       "0   I'm reading a lot of reviews saying that this ...  \n",
       "1   This soundtrack is my favorite music of all ti...  \n",
       "2   I truly like this soundtrack and I enjoy video...  \n",
       "3   If you've played the game, you know how divine...  \n",
       "4   I am quite sure any of you actually taking the...  \n",
       "5   This is a self-published book, and if you want...  \n",
       "6   I loved Whisper of the wicked saints. The stor...  \n",
       "7   I just finished reading Whisper of the Wicked ...  \n",
       "8   This was a easy to read book that made me want...  \n",
       "9   A complete waste of time. Typographical errors...  \n",
       "10  This was a great book,I just could not put it ...  \n",
       "11  I thought this book was brilliant, but yet rea...  \n",
       "12  I guess you have to be a romance novel lover f...  \n",
       "13  I feel I have to write to keep others from was...  \n",
       "14  It's glaringly obvious that all of the glowing...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f713d31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configs \n",
    "MAX_SEQUENCE_LENGTH = 300\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "TEST_BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00fbcdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to convert text into suitable Bert input form\n",
    "def get_model_inputs(str1, str2, _truncation_strategy, length, tokenizer, pad_seq = True):\n",
    "\n",
    "    inputs = tokenizer.encode_plus(str1,\n",
    "                                    str2,\n",
    "                                    add_special_tokens=True,\n",
    "                                    max_length=length,\n",
    "                                    truncation_strategy=_truncation_strategy,\n",
    "                                    pad_to_max_length=pad_seq)\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    input_masks = inputs[\"attention_mask\"]\n",
    "    input_segments = inputs[\"token_type_ids\"]\n",
    "    return [input_ids, input_masks, input_segments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a401a231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datagen(dataset, batch_size):\n",
    "  data = dataset.copy()\n",
    "  while True:\n",
    "    for i in range(1):\n",
    "      inputs_x_id, inputs_x_mask, inputs_x_segment, inputs_y = [], [], [], []\n",
    "      start = i*batch_size\n",
    "      end = start+batch_size\n",
    "      batch_x = data.iloc[start:end,[0, 1]].values\n",
    "      batch_y = data.iloc[start:end,2].values\n",
    "      for i in range(batch_size):\n",
    "        input_ids, input_masks, input_segments = get_model_inputs(batch_x[i,0], batch_x[i,1], \n",
    "                                                                  truncation_strategy, \n",
    "                                                                  MAX_SEQUENCE_LENGTH, \n",
    "                                                                  Tokenizer)\n",
    "        \n",
    "        inputs_x_id.append(input_ids)\n",
    "        inputs_x_mask.append(input_masks)\n",
    "        inputs_x_segment.append(input_segments)\n",
    "        inputs_y.append(batch_y[i])\n",
    "\n",
    "      yield ([np.array(inputs_x_id, dtype=np.int32),\n",
    "             np.array(inputs_x_mask, dtype=np.int32),\n",
    "             np.array(inputs_x_segment, dtype=np.int32)],\n",
    "             np.array(inputs_y, dtype=np.int32))\n",
    "\n",
    "K.clear_session()\n",
    "train_data_generator = datagen(train_dataset, TRAIN_BATCH_SIZE)\n",
    "test_data_generator = datagen(test_dataset, TEST_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8a231540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file tf_model.h5 from cache at C:\\Users\\Borislav/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\0a6aa9128b6194f4f3c4db429b6cb4891cdb421b\\tf_model.h5\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    input_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "    input_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "    input_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "    config = BertConfig() \n",
    "    config.output_hidden_states = False # Set to True to obtain hidden states\n",
    "    \n",
    "    bert_model = TFBertModel.from_pretrained('bert-base-uncased', config=config)\n",
    "    \n",
    "    # if config.output_hidden_states = True, obtain hidden states via bert_model(...)[-1]\n",
    "    input_embedding = bert_model(input_id, attention_mask=input_mask, token_type_ids=input_atn)[0]\n",
    "\n",
    "    # Get average tokens output\n",
    "    tokens_embedding = tf.keras.layers.GlobalAveragePooling1D()(input_embedding)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(tokens_embedding)\n",
    "        \n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=[input_id, input_mask, input_atn], outputs=x)\n",
    "    \n",
    "    return model\n",
    "\n",
    "bert = create_model();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4e8f1ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#loss function (modified binary cross entropy loss function which gives higher attention to misclassified examples)\n",
    "def focal_loss(y_true, y_pred, gamma=2., alpha=.25):\n",
    "    pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "    pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "    return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1b2ef696",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor= 'val_acc', \n",
    "                               mode = 'max',\n",
    "                               patience=30, \n",
    "                               verbose=1)\n",
    "\n",
    "model_checkpoint = ModelCheckpoint('BERT_MODEL_AMAZON_REVIEW_CLASSIFIER',\n",
    "                                   monitor = 'val_acc', \n",
    "                                   mode = 'max', \n",
    "                                   save_best_only=True, \n",
    "                                   verbose=1)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', \n",
    "                              mode = 'max',\n",
    "                              factor=0.2, \n",
    "                              patience=4, \n",
    "                              min_lr=0.0000001, \n",
    "                              verbose=1)\n",
    "opt = Adam(learning_rate = 0.0005)\n",
    "bert.compile(loss = focal_loss, optimizer= opt, metrics=['acc'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bbed6c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Borislav\\AppData\\Local\\Temp\\ipykernel_6684\\3838879103.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = bert.fit_generator(generator=train_data_generator,\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'truncation_strategy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6684\\3838879103.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m history = bert.fit_generator(generator=train_data_generator,\n\u001b[0m\u001b[0;32m      3\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_data_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mTRAIN_BATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mTEST_BATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2505\u001b[0m             \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2506\u001b[0m         )\n\u001b[1;32m-> 2507\u001b[1;33m         return self.fit(\n\u001b[0m\u001b[0;32m   2508\u001b[0m             \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2509\u001b[0m             \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6684\\78581510.py\u001b[0m in \u001b[0;36mdatagen\u001b[1;34m(dataset, batch_size)\u001b[0m\n\u001b[0;32m     10\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         input_ids, input_masks, input_segments = get_model_inputs(batch_x[i,0], batch_x[i,1], \n\u001b[1;32m---> 12\u001b[1;33m                                                                   \u001b[0mtruncation_strategy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m                                                                   \u001b[0mMAX_SEQUENCE_LENGTH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                                                                   Tokenizer)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'truncation_strategy' is not defined"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "history = bert.fit_generator(generator=train_data_generator,\n",
    "                    validation_data=test_data_generator,\n",
    "                    steps_per_epoch = len(train_dataset)//TRAIN_BATCH_SIZE,\n",
    "                    validation_steps = len(test_dataset)//TEST_BATCH_SIZE,\n",
    "                    epochs = 200,\n",
    "                    callbacks = [early_stopping, model_checkpoint, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55b0135",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "389e965860e72a2262259afb6517e3f7a21b0e2f45634ac26ff8f1e7a7de5348"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
